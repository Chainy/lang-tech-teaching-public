{"cells": [{"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/html": ["<style>/* Style des gesamten Dokuments */\n", "#notebook-container {\n", "\tfont-family: \"NimbusMonL-ReguObli\";\n", "\tfont-size: 120%\n", "}\n", "\n", "/* Style f\u00c3\u00bcr die \u00c3\u0153berschrift: Zentriert diese und stellt sie fett dar. */\n", ".headline {\n", "\ttext-align: center;\n", "\tfont-weight: bold;\n", "\tfont-size: 185.7%\n", "}\n", "\n", "/* Style f\u00c3\u00bcr die Aufgabenbeschreibung. Z.B.: \"\u00c3\u0153bung zum Thema...\" */\n", ".description {\n", "\ttext-align: center;\n", "\tfont-size: 145.7%\n", "}\n", "\n", "/* Hebt das Abgabedatum fett und kursiv hervor */\n", "#submission {\n", "\tfont-weight: bold;\n", "}\n", "\n", "/* Style f\u00c3\u00bcr das eigentliche Thema. Z.B.: \"Intelligenz\" */\n", "#topic {\n", "\tfont-style: italic;\n", "}\n", "\n", ".task_description {\n", "\tmargin-bottom: 20px;\n", "}\n", "\n", "/* Hebt die Aufgabennummerierung fett hervor. */\n", ".task {\n", "\tfont-style: normal;\n", "\tfont-weight: bold;\n", "\tfont-size: 120%;\n", "\tborder-bottom: 2px solid black;\n", "  background-color: #97CAEF;\n", "  color: black;\n", "\tpadding: 2px;\n", "  padding-left: 50px;\n", "  padding-right: 50px;\n", "}\n", "\n", ".subtask {\n", "\tfont-style: normal;\n", "\tfont-size: 100%;\n", "  background-color: #CAFAFE;\n", "  color: black;\n", "\tpadding: 2px;\n", "  padding-left: 25px;\n", "  padding-right: 25px;\n", "}\n", "\n", ".l1 {\n", "\tfont-style: normal;\n", "\tfont-size: 100%;\n", "  background-color: #14A76C;\n", "  color: black;\n", "\tpadding: 2px;\n", "  padding-left: 5px;\n", "  padding-right: 5px;\n", "}\n", "\n", ".l2 {\n", "\tfont-style: normal;\n", "\tfont-size: 100%;\n", "  background-color: #FFE400;\n", "  color: black;\n", "\tpadding: 2px;\n", "  padding-left: 5px;\n", "  padding-right: 5px;\n", "}\n", "\n", ".l3 {\n", "\tfont-style: normal;\n", "\tfont-size: 100%;\n", "  background-color: #FF652F;\n", "  color: black;\n", "\tpadding: 2px;\n", "  padding-left: 5px;\n", "  padding-right: 5px;\n", "}\n", "\n", ".points {\n", "\tfont-style: italic;\n", "}\n", "\n", "ol.lower_roman {\n", "    list-style-type: lower-roman;\n", "}\n", "\n", "ol.characters {\n", "    list-style-type: lower-alpha;\n", "}\n", "\n", "/* Style einer Code-Cell */\n", ".CodeMirror-code {\n", "\tbackground-color: #ededed\n", "}\n", "\n", "/* Style eines Kommentars im Code \u00c3\u00a4ndern. */\n", ".cm-s-ipython span.cm-comment {\n", "\n", "}\n", "\n", ".cm-s-ipython span.cm-atom {\n", "\n", "}\n", "\n", ".cm-s-ipython span.cm-number {\n", "\n", "}\n", "\n", "/* Style eines Python-Keywords \u00c3\u00a4ndern */\n", ".cm-s-ipython span.cm-keyword {\n", "\tcolor: #B000B0\n", "}\n", "\n", ".cm-s-ipython span.cm-def {\n", "\n", "}\n", "\n", "/* Style einer Python-Variable \u00c3\u00a4ndern */\n", ".cm-s-ipython span.cm-variable {\n", "\n", "}\n", "\n", "/* Style einer Property \u00c3\u00a4ndern */\n", ".cm-s-ipython span.cm-property {\n", "\n", "}\n", "\n", "/* Style eines Python-Operators \u00c3\u00a4ndern */\n", ".cm-s-ipython span.cm-operator {\n", "\n", "}\n", "\n", "/* Style eines Python-Strings \u00c3\u00a4ndern */\n", ".cm-s-ipython span.cm-string {\n", "\tcolor: brown;\n", "}\n", "\n", "/* Style einer eingebauten Funktion \u00c3\u00a4ndern (z.B. \"open\") */\n", ".cm-s-ipython span.cm-builtin {\n", "\n", "}\n", "\n", "/* Hebt hervor, welche Klammern zueinander passen */\n", ".cm-s-ipython .CodeMirror-matchingbracket {\n", "\n", "}\n", "\n", ".cm-s-ipython span.cm-variable-2 {\n", "\n", "}\n", "</style>"], "text/plain": ["<IPython.core.display.HTML object>"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["from IPython.core.display import HTML\n", "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"headline\">\n", "Language Technology / Sprachtechnologie\n", "<br><br>\n", "Wintersemester 2020/2021\n", "</div>\n", "<br>\n", "<div class=\"description\">\n", "    \u00dcbung zum Thema <i id=\"topic\">\"Sentiment Analysis\"</i>\n", "    <br><br>\n", "    Deadline Abgabe: <i #id=\"submission\">12.02.2021</i>\n", "</div>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "pd.set_option('display.max_colwidth', None)\n", "import matplotlib.pyplot as plt\n", "%matplotlib inline\n", "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n", "from os.path import join\n", "from nltk.corpus import stopwords"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv(join('data', 'amazon_reviews_sentiments.tsv'), names=['Statement', 'label'], sep='\\t')\n", "\n", "statements = df['Statement']\n", "labels = df['label']\n", "\n", "train_statements = statements[:-200]\n", "train_labels = labels[:-200]\n", "test_statements = statements[-200:]\n", "test_labels = labels[-200:]\n", "df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Pr\u00e4senz\u00fcbung"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Try with tf-IDF "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 11.5:</i> <br>\n", "</div>\n", "\n", "Train the naive bayes classifier using the TF_IDF feature set of the train data. <br>\n", "Then test the classifier's performance on the test set.  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.5.1</i> <i class=\"l1\">L1</i> <br>\n", "</div>\n", "\n", "Explain the following code.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import TfidfVectorizer\n", "\n", "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), lowercase=True)  \n", "features_train = vectorizer.fit_transform(train_statements).toarray()\n", "features_test = vectorizer.transform(test_statements).toarray() \n", "print(features_train)\n", "\n", "X_train = features_train\n", "y_train = train_labels\n", "X_test = features_test\n", "y_test = test_labels\n", "X_train.shape"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>\n", "\n", "Create tf-idf features, remove nltk english stopwords,  lowercasing for train and test statements."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.5.2</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Apply the naive bayes algorithm and train the classifier using the train set using the 'fit' method. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n", "\n", "clf = GaussianNB()\n", "# TODO train the classifier on the train set.\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["from sklearn.naive_bayes import GaussianNB\n", "\n", "# fast better classifier\n", "clf = GaussianNB()\n", "clf.fit(X_train, y_train)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Try naive bayes algorithm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.5.3</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Evaluate the classifier on the test set using confusion metric, accuracy and classification report."]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["predictions = clf.predict(X_test)\n", "\n", "cf = confusion_matrix(y_test,predictions)\n", "print(cf)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["import seaborn as sns\n", "sns.heatmap(cf, cmap=\"GnBu\", annot=True, fmt='g');"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["accuracy_score(y_test, predictions) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["print(classification_report(y_test,predictions))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.5.4</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Find three statements that the classifier fails. Please run the following code to test the classifier. Give a justification for the classifier failure. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Process user input. Processing continues until the user says goodbye. \n", "text = \"\"\n", "while text != \"goodbye\":\n", "    # Read user input\n", "    text = input()\n", "    test_features = vectorizer.transform([text]).toarray()  \n", "    print('Positive Sentiment' if clf.predict(test_features) == 1 else 'Negative sentiment')"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["i am sad \u2013\u2013 *sad* has a low tf-idf score for negative sentiment text\n", "\n", "product is shit \u2013\u2013 *shit* is unknown to classifier\n", "\n", "i am not satisfied with the product \u2013\u2013 *not* is is not considered as removed in stopwords"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.5.5</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "The following code snippet generates false positives and false negatives. Please observe the classifier's first ten wrong judgements and describe them briefly."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# checkout false positive examples\n", "\n", "no_example = 10\n", "\n", "\n", "for i, sample in enumerate(X_test):\n", "    if y_test.values[i] != int(clf.predict([sample])[0]):\n", "        print(\"Text: %s\" %test_statements.values[i])\n", "        print(\"Ground Truth: %s\" %y_test.values[i])\n", "        print(\"Prediction: %s\\n\" %clf.predict([sample])[0])\n", "        no_example -= 1\n", "        if no_example == 0:\n", "            break\n", "    "]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>\n", "\n", "The classifier doesn't work very well with negation in the statements\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Try with word embeddings -word2vec"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 11.6:</i> <br>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.1</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Downloaded the pretrained google news word2vec word embeddings from <br><br> https://drive.google.com/uc?export=download&id=0B7XkCwpI5KDYNlNUTTlSS21pQmM <br><br> Run the folloing code and explain it."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gensim\n", "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) #limit"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["The gensim module is loading the google word2vec model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.2</i> <i class=\"l3\">L3</i> <br>\n", "</div>\n", "\n", "Generate the embeddding features for test statements. Please remove stopwords (see the train statements example)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embedding_features_train = []\n", "for sent in train_statements:\n", "    sent = [x.lower() for x in sent.split(' ') if x not in stopwords.words('english')]\n", "    for i in range(len(sent)):\n", "        sent[i] = model[sent[i]] if sent[i] in model.vocab else np.zeros(300)\n", "    embedding_features_train.append(np.mean(sent, axis=0))\n", "\n", "embedding_features_train = np.array(embedding_features_train)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["embedding_features_test = []\n", "for sent in test_statements:\n", "    #TODO\n", "\n", "embedding_features_test = np.array(embedding_features_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train_emb = embedding_features_train\n", "y_train_emb = train_labels\n", "\n", "X_test_emb = embedding_features_test\n", "y_test_emb = test_labels"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["embedding_features_test = []\n", "for sent in test_statements:\n", "    sent = [x.lower() for x in sent.split(' ') if x not in stopwords.words('english')]\n", "    for i in range(len(sent)):\n", "        sent[i] = model[sent[i]] if sent[i] in model.vocab else np.zeros(300)\n", "    embedding_features_test.append(np.mean(sent, axis=0))\n", "    \n", "embedding_features_test = np.array(embedding_features_test)    "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["X_train_emb = embedding_features_train\n", "y_train_emb = train_labels\n", "\n", "X_test_emb = embedding_features_test\n", "y_test_emb = test_labels"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.3</i> <i class=\"l3\">L3</i> <br>\n", "</div>\n", "\n", "Train the naive bayes classifier and evaluate it using confusion metric, accuracy and classification report. "]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["clf_emb = GaussianNB()\n", "clf_emb.fit(X_train_emb, y_train_emb)\n", "\n", "predictions_emb = clf_emb.predict(X_test_emb)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["cf_emb = confusion_matrix(y_test_emb,predictions_emb)\n", "print(cf_emb)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": ["solution"]}, "outputs": [], "source": ["sns.heatmap(cf_emb, cmap=\"GnBu\", annot=True, fmt='g');"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["accuracy_score(y_test_emb, predictions_emb) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["print(classification_report(y_test_emb,predictions_emb))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.4</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Find three statements that the classifier fails.<br> Please run the following code snippet to test the classifier. Give a justification for the classifier failure. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def get_embedding_vector(input_text):\n", "    dim = 300\n", "    return np.mean([model[w] for w in input_text.split(' ') if w in model.vocab]\n", "                            or [np.zeros(dim)], axis=0)\n", "\n", "\n", "# Process user input. Processing continues until the user says goodbye. \n", "text = \"\"\n", "while text != \"goodbye\":\n", "    # Read user input\n", "    text = input()\n", "    test_features_emb = get_embedding_vector(text) \n", "    print('Positive Sentiment' if clf_emb.predict([test_features_emb]) == 1 else 'Negative sentiment')\n"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>\n", "\n", "i am not sad \u2013\u2013 negation\n", "\n", "product quality is okay okay \u2013\u2013 interjection words\n", "\n", "i hardly like the product \u2013\u2013 hardly\n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.5</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "The error analysis shows that despite a better performance model it is not working well with sentences having a negation. Understand and explain the idea behind the following function. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def toggle_prediction(prediction, text):\n", "    prediction_toggled = [x for x in prediction]\n", "    for i, sent in enumerate(text):\n", "        sent = sent.lower().split(' ')\n", "        if 'not' in sent:\n", "            prediction_toggled[i] = 1 - prediction_toggled[i]\n", "        else:\n", "            for w in sent:\n", "                if \"n't\" in w:\n", "                    prediction_toggled[i] = 1 - prediction_toggled[i]\n", "                    break\n", "    return prediction_toggled\n", "                \n", "predictions_emb_toggled = toggle_prediction(predictions_emb, test_statements)"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["The function toggles the prediction if the test statement contains negations.\n", "For example words like \"not\", \"haven't\", \"aren't\" etc."]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.6</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Compare the performance with the original classfier prediction results and describe your observation."]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["cf_emb = confusion_matrix(y_test_emb,predictions_emb_toggled)\n", "print(cf_emb)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["sns.heatmap(cf_emb, cmap=\"GnBu\", annot=True, fmt='g')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["accuracy_score(y_test_emb,predictions_emb_toggled) "]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["print(classification_report(y_test_emb, predictions_emb_toggled))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.6.7</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Find three statements that the negation conditioned classifier fails. Please run the following code snippet to test the classifier. Give justifications for the classifier failure. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Process user input. Processing continues until the user says goodbye. \n", "text = \"\"\n", "while text != \"goodbye\":\n", "    # Read user input\n", "    text = input()\n", "    test_features_emb = get_embedding_vector(text) \n", "    print('Positive Sentiment' if toggle_prediction(clf_emb.predict([test_features_emb]), [text])[0] == 1 else 'Negative sentiment')"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>\n", "\n", "1. Does Not Work.\n", "2. My car will not accept this cassette.\n", "3. It's not what it says it is."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Error Analysis"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "    <i class=\"task\">Task 11.7:</i> <br>\n", "</div>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<div class=\"task_description\">\n", "   <i class=\"subtask\">11.7.1</i> <i class=\"l2\">L2</i> <br>\n", "</div>\n", "\n", "Please go through the following code snippets. Which of them shows where toggling of prediction works and where not. Can you provide a comment based on the code results?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["no_example = 10\n", "\n", "for i, sample in enumerate(X_test_emb):\n", "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n", "        if y_test_emb.values[i] != clf_emb.predict([sample])[0]:\n", "            print(\"Text: %s\" %test_statements.values[i])\n", "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n", "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n", "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n", "            print()\n", "            no_example -= 1\n", "            if no_example == 0:\n", "                break"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["no_example = 10\n", "\n", "for i, sample in enumerate(X_test_emb):\n", "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n", "        if y_test_emb.values[i] != toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]]):\n", "            print(\"Text: %s\" %test_statements.values[i])\n", "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n", "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n", "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n", "            print()\n", "            no_example -= 1\n", "            if no_example == 0:\n", "                break"]}, {"cell_type": "markdown", "metadata": {"tags": ["solution"]}, "source": ["<strong style=\"color: blue\">L\u00f6sung:</strong>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["# toggling of prediction works\n", "\n", "no_example = 10\n", "\n", "\n", "for i, sample in enumerate(X_test_emb):\n", "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n", "        if y_test_emb.values[i] != clf_emb.predict([sample])[0]:\n", "            print(\"Text: %s\" %test_statements.values[i])\n", "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n", "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n", "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n", "            print()\n", "            no_example -= 1\n", "            if no_example == 0:\n", "                break"]}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": ["solution"]}, "outputs": [], "source": ["# toggling of prediction fails\n", "\n", "no_example = 10\n", "\n", "\n", "for i, sample in enumerate(X_test_emb):\n", "    if \"n't\" in test_statements.values[i].lower() or \"not\" in test_statements.values[i].lower():\n", "        if y_test_emb.values[i] != int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]):\n", "            print(\"Text: %s\" %test_statements.values[i])\n", "            print(\"Ground Truth: %s\" %y_test_emb.values[i])\n", "            print(\"Prediction: %s\" %clf_emb.predict([sample])[0])\n", "            print(\"Toggled Prediction: %s\" %int(toggle_prediction(clf_emb.predict([sample]), [test_statements.values[i]])[0]))\n", "            print()\n", "            no_example -= 1\n", "            if no_example == 0:\n", "                break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Homework\n", "<br>\n", "\n", "<div class=\"task_description\">\n", "    <i class=\"task\">Task 11.1:</i> <br>\n", "</div>\n", "\n", "\n", "You have seen how to solve the task by using the Word2Vec embeddings. Now try it yourself using the GloVe embeddings and compare the results. Elaborate your comparison briefly. Comparing only the accuracy is not enough.\n", "\n", "We recommend using the glove.6B file you know from other exercises.\n", "<a href=\"https://nlp.stanford.edu/projects/glove/\">GloVe Website</a>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### *Submission guidelines*\n", "* *The submission has to be done by a team of two people. **Individual submissions will not be graded**.*\n", "* *Please state the **name and matriculation number of all team members** in every submission **clearly**.*\n", "* *Only **one team member should submit** the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.*\n", "* *The submission must be in a Jupyter Notebook format (.ipynb). Submissions in **other formats will not be graded**.*\n", "* *It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.*\n", "* *If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: ``![example1](examplepicture1.PNG)`` (without apostrophs in a Markdown-Cell).*"]}], "metadata": {"celltoolbar": "Tags", "kernelspec": {"display_name": "nlp-env", "language": "python", "name": "nlp-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.6"}, "varInspector": {"cols": {"lenName": 16, "lenType": 16, "lenVar": 40}, "kernels_config": {"python": {"delete_cmd_postfix": "", "delete_cmd_prefix": "del ", "library": "var_list.py", "varRefreshCmd": "print(var_dic_list())"}, "r": {"delete_cmd_postfix": ") ", "delete_cmd_prefix": "rm(", "library": "var_list.r", "varRefreshCmd": "cat(var_dic_list()) "}}, "types_to_exclude": ["module", "function", "builtin_function_or_method", "instance", "_Feature"], "window_display": false}}, "nbformat": 4, "nbformat_minor": 2}