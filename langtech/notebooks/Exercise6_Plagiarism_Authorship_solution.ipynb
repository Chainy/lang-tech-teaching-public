{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Style des gesamten Dokuments */\n",
       "#notebook-container {\n",
       "\tfont-family: \"NimbusMonL-ReguObli\";\n",
       "\tfont-size: 120%\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r die Ãœberschrift: Zentriert diese und stellt sie fett dar. */\n",
       ".headline {\n",
       "\ttext-align: center;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 185.7%\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r die Aufgabenbeschreibung. Z.B.: \"Ãœbung zum Thema...\" */\n",
       ".description {\n",
       "\ttext-align: center;\n",
       "\tfont-size: 145.7%\n",
       "}\n",
       "\n",
       "/* Hebt das Abgabedatum fett und kursiv hervor */\n",
       "#submission {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r das eigentliche Thema. Z.B.: \"Intelligenz\" */\n",
       "#topic {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       ".task_description {\n",
       "\tmargin-bottom: 20px;\n",
       "}\n",
       "\n",
       "/* Hebt die Aufgabennummerierung fett hervor. */\n",
       ".task {\n",
       "\tfont-style: normal;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 120%;\n",
       "\tborder-bottom: 2px solid black;\n",
       "  background-color: #97CAEF;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 50px;\n",
       "  padding-right: 50px;\n",
       "}\n",
       "\n",
       ".subtask {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #CAFAFE;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 25px;\n",
       "  padding-right: 25px;\n",
       "}\n",
       "\n",
       ".l1 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #14A76C;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l2 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FFE400;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l3 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FF652F;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".points {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       "ol.lower_roman {\n",
       "    list-style-type: lower-roman;\n",
       "}\n",
       "\n",
       "ol.characters {\n",
       "    list-style-type: lower-alpha;\n",
       "}\n",
       "\n",
       "/* Style einer Code-Cell */\n",
       ".CodeMirror-code {\n",
       "\tbackground-color: #ededed\n",
       "}\n",
       "\n",
       "/* Style eines Kommentars im Code Ã¤ndern. */\n",
       ".cm-s-ipython span.cm-comment {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-atom {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-number {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Keywords Ã¤ndern */\n",
       ".cm-s-ipython span.cm-keyword {\n",
       "\tcolor: #B000B0\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-def {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Python-Variable Ã¤ndern */\n",
       ".cm-s-ipython span.cm-variable {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Property Ã¤ndern */\n",
       ".cm-s-ipython span.cm-property {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Operators Ã¤ndern */\n",
       ".cm-s-ipython span.cm-operator {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Strings Ã¤ndern */\n",
       ".cm-s-ipython span.cm-string {\n",
       "\tcolor: brown;\n",
       "}\n",
       "\n",
       "/* Style einer eingebauten Funktion Ã¤ndern (z.B. \"open\") */\n",
       ".cm-s-ipython span.cm-builtin {\n",
       "\n",
       "}\n",
       "\n",
       "/* Hebt hervor, welche Klammern zueinander passen */\n",
       ".cm-s-ipython .CodeMirror-matchingbracket {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-variable-2 {\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2020/2021\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Plagiarism / Authorship\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Friday, 18.12.2020 (11:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import inaugural, brown, shakespeare, genesis, gutenberg\n",
    "from nltk.book import*\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "When we want to analyse the typical fingerprint of writers, we are often interested in features that are independent from a specific genre or topic.\n",
    " \n",
    "Which of the following features do you think are most topic-dependent and why?\n",
    "* Most frequent verbs\n",
    "* Most frequent prepositions\n",
    "* Most frequent named entities\n",
    "* Most frequent pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>\n",
    "\n",
    "Named entries are probably the most topic-dependent of the four features, to a similar extent verbs, pronouns maybe less so (but compare a novel in first person to technical documentation by the same author), even prepositions can be topic dependent but might be less affected by topic. (Those are the general considerations, for a more definite answer one would have to try it for each respective author.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type-Token Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.2:</i> <i class=\"l3\">L3</i><br>\n",
    "</div>\n",
    "Type-Token-Ratio is a measure of lexical diversity and computed by dividing the number of different types by the overall number of tokens in a text. <br> Implement a method that computes the type-token-ratio for an input text.\n",
    "What happens if you apply this method to the first 100, 500, 1000, 2000, 10000 and 20000 word in Romeo and Julia (Shakespeare corpus)? <br> Can you explain your observation?\n",
    "Discuss what you could do about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def computeTTR(numberOfWords):\n",
    "    fd = nltk.FreqDist(w1 for w1 in shakespeare.words('r_and_j.xml')[:numberOfWords])\n",
    "    tokens = fd.N()\n",
    "    types = fd.B()\n",
    "    print (tokens,\"\\t\",types,\"\\t\"+str(types/tokens))\n",
    "\n",
    "computeTTR(100)\n",
    "computeTTR(200)\n",
    "computeTTR(500)\n",
    "computeTTR(1000)\n",
    "computeTTR(2000)\n",
    "computeTTR(10000)\n",
    "computeTTR(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We observe that TTR is strongly text length dependent.<br>\n",
    "What you can do about it: Only compare texts of the same length or downsample texts in such a way that they have a comparable length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.3:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.3.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. If executed, what will be the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in brown.words() if w.startswith('en')]\n",
    "print(FreqDist(words).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>\n",
    "\n",
    "The output of the code are the ten most frequent words from the brown corpus that start with the prefix ”en” (case sensitive). <br> For the Brown corpus: [enough, end, entire, entered, energy, entirely, enemy, enter, ends, ended]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.3.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Change the code above to that it gets the ten most frequent words from the corpus that start with the prefix ”en”, end with the suffix ”ed” and are at least five characters long. Moreover, it should not matter if the word is written in upper or lower case (”Ended” and ”ended” should be regarded as the same word)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "words = [w.lower() for w in brown.words() if w.lower().startswith('en') and w.lower().endswith('ed') and len(w) >= 5]\n",
    "print(FreqDist(words).most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "For the Brown Corpus the output is: [entered, ended, enjoyed, entitled, engaged, encountered, encouraged, enforced, enabled, enacted]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.3.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Using the Brown corpus, find all words that consist of two other words that are at least 5 characters long, e.g.”storehouse”, but not “infrequent” (in + frequent), as “in” is only 2 characters long. Can you think of an improvement that finds surprising ones like “horizontally” (horizon + tally) instead of normal compositions like “storehouse” (store + house)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "words = FreqDist([w.lower() for w in brown.words() if len(w) >= 5])\n",
    "for token in words.keys():\n",
    "    for start in words.keys():\n",
    "        if token.startswith(start):\n",
    "            rest = token[(len(start)):(len(token))]\n",
    "            for end in words.keys():\n",
    "                if rest == end:\n",
    "                    print(token, \"(\", start,\",\", end,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.4:</i> <br>\n",
    "</div>\n",
    "\n",
    "\n",
    "An important problem in computational linguistics is morphological analysis.<br> This consists of breaking down a word into its component pieces, for example losses might be broken down as loss + es. In English, morphology is relatively simple and is mostly comprised of prefixes and suffixes. To get an idea of what suffixes are common in English (and thus could be morphemes), we can look at the frequencies of the last two characters of sufficiently long words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.4.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Why is it not suitable for finding the most frequent two-character suffixes in a corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = [w.lower() for w in brown.words() if w.lower().endswith('en') or w.lower().endswith('ly')]\n",
    "print(FreqDist(suffixes).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong> <br>\n",
    "\n",
    "It is not suitable due to the following reasons: <br>\n",
    "The code takes only words with the suffixes ”ly” or ”en” into consideration. The output is a full word instead of a suffix.<br>\n",
    "The output should contain more than one suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.4.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Improve the code above so that only token with a length of at least five characters are taken into consideration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "suffixes = [w.lower() for w in brown.words() if (len(w))>=5 and (w.lower().endswith('en') or w.lower().endswith('ly'))]\n",
    "print(FreqDist(suffixes).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.4.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Improve the code further so that it returns the 20 most frequent two-character suffixes from words with at least five characters from a corpus. <br>\n",
    "As a sanity test, you may want test your code on the words in Sense and Sensibility. If you pass the words of a novel (or any sufficiently large English document) as the argument to your code, you should see some common English suffixes in the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "suffixes = [w.lower()[-2:] for w in brown.words() if (len(w))>=5]\n",
    "print(FreqDist(suffixes).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "Top suffixes in Moby Dick:[ed, ng, er, es, le, ly, re, se, st, on, rs, nt, nd, ce, ht, ts, in, en, al, ss]\n",
    "\n",
    "Top suffixes in Sense and Sensibility:[ed, ng, er, on, ly, ld, nt, ce, re, le, se, es, st, or, ry, ne, ch, ss, ty, ht]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.5:</i> <br>\n",
    "</div>\n",
    "\n",
    "Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of\n",
    "appropriate difficulty for language learners. <br>\n",
    "The Automated Readability Index (ARI) of a text is defined to be:<br>\n",
    "``ARI = 4.71μw + 0.5μs − 21.43``\n",
    "Where ``μw`` is the average number of letters per word, and ``μs`` is the average number of words per sentence, in a given text. <br>\n",
    "As a rough guide, a score of 1 corresponds to the reading level at an age of 6 to 8, a score of 8 corresponds to the typical reading level of a 14 year-old US child. A score of 12 corresponds to the reading level of a 17 year-old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "What does the function below compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = len(sentences)\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    \n",
    "    print(sentCount)\n",
    "\n",
    "calculate_readability(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>\n",
    "\n",
    "The function counts the number of sentences in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Enhance the function above so that the number of words is added up in the variable wordCount (if you got two sentences, one with 5 words, one with 8, wordCount should be 13)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = len(sentences)\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    for sentence in sentences:\n",
    "        wordCount += len(sentence)\n",
    "    print (sentCount)\n",
    "    print (wordCount)\n",
    "    \n",
    "calculate_readability(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.3</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Enhance the function again, so that in charCount the number of characters from each word is added up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = len(sentences)\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    for sentence in sentences:\n",
    "        wordCount += len(sentence)\n",
    "        for word in sentence:\n",
    "            charCount += len(word)\n",
    "    print (sentCount)\n",
    "    print (wordCount)\n",
    "    print (charCount)\n",
    "\n",
    "calculate_readability(brown.sents())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now change your function, so that it returns a double with the value of the ARI score of the corpus (Hint: print is not a return statement). Then compute the ARI score of the Brown corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = len(sentences)\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    for sentence in sentences:\n",
    "        wordCount += len(sentence)\n",
    "        for word in sentence:\n",
    "            charCount += len(word)\n",
    "    return 4.71 * charCount / wordCount + 0.5 * wordCount / sentCount - 21.43\n",
    "\n",
    "print (\"Brown Corpus ARI: \", calculate_readability(nltk.corpus.brown.sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "For the Brown corpus, the ARI score is 8.8, what approximately corresponds to the reading level of a 14 year old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.5</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Use the inaugural corpus and compare it to the Brown corpus to prove the hypothesis: “Speeches are easier to understand than news.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = len(sentences)\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    for sentence in sentences:\n",
    "        wordCount += len(sentence)\n",
    "        for word in sentence:\n",
    "            charCount += len(word)\n",
    "    return 4.71 * charCount / wordCount + 0.5 * wordCount / sentCount - 21.43\n",
    "\n",
    "# 14.1\n",
    "print ('Inaugural Speeches ARI:', calculate_readability(inaugural.sents()))\n",
    "# 10.2\n",
    "print ('News category in the brown corpus ARI:', calculate_readability(brown.sents(categories=['news'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "We can notice that the readability index for inaugural speeches (14.2) is higher than that for the news text (8.8). This observation contradicts the hypothesis. Can you think of reasons for that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.5.6</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Compute the readability score of all inaugural speeches. Plot it. Discuss the changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def calculate_readability(sentences):\n",
    "    sentCount = 0\n",
    "    wordCount = 0\n",
    "    charCount = 0\n",
    "    for sentence in sentences:\n",
    "        sentCount += 1\n",
    "        wordCount += len(sentence)\n",
    "\n",
    "        for word in sentence:\n",
    "            charCount += len(word)\n",
    "\n",
    "    return 4.71 * charCount / wordCount + 0.5 * wordCount / sentCount - 21.43\n",
    "\n",
    "# \"hijack\" the conditional frequency distribution for storing the ARI scores\n",
    "ARI_scores = nltk.ConditionalFreqDist()\n",
    "\n",
    "for fileid in inaugural.fileids():\n",
    "    ARI_scores['ARI'][fileid] = calculate_readability(inaugural.sents(fileid))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.rcParams.update({'font.size': 22})\n",
    "ARI_scores.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation is that differences between speeches are large, e.g. compare the 1797 speech of Washington with those of its successor Adams (a score of almost 35 – incredibly difficult). However, there is a clear tendency that inaugural speeches get easier over time. Speeches before 1900 seldom dropped below a score of 15. In contrast, some speeches like Johnson 1965 and Bush Sr. 1989 are close to 5 which corresponds to the reading level of a 12 year old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 6.6.:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.6.1.</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Look at the code below. Describe what the function does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mostCommon(n):\n",
    "    mc = FreqDist(text1)\n",
    "    words = []\n",
    "    for m in mc.most_common(n):\n",
    "        if len(m[0]) ==3:\n",
    "            words.append(m)\n",
    "    return words\n",
    "\n",
    "mostCommon(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The function returns a list with all tokens from a corpus that are amongst the n most frequent words of the corpus and have a length of three characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.6.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Enhance the function shorten (c,n,part) to process a text, omitting the n most frequently occurring words of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moby = text1\n",
    "sense = text2\n",
    "chat = text5\n",
    "def shorten(c,n,part):#(corpus, number, test on)\n",
    "    #todo\n",
    "    \n",
    "print(\"moby\",shorten(moby,100, moby[42:73]),\"\\n\")\n",
    "print(\"sense and sensibility\",shorten(sense,100, sense[22:68]),\"\\n\")\n",
    "print(\"chat\",shorten(chat,100, chat[:100]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "moby = text1\n",
    "sense = text2\n",
    "chat = text5\n",
    "\n",
    "\n",
    "def shorten(c,n,part):\n",
    "    mc = FreqDist(c)\n",
    "    mostCommon = [y[0] for y in mc.most_common(n)]\n",
    "    words = []\n",
    "    for m in part:\n",
    "        if m not in mostCommon:\n",
    "            words.append(m)\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "print(\"moby\",shorten(moby,100, moby[42:73]),\"\\n\")\n",
    "print(\"sense and sensibility\",shorten(sense,100, sense[22:68]),\"\\n\")\n",
    "print(\"chat\",shorten(chat,100, chat[:100]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.6.3.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Enhance your code so that results not only for one but for various values of n (e.g. 10, 100, 500) are shown. Experiment with the values of n to find a useful limit for shortening a text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung: </strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "moby = text1\n",
    "sense = text2\n",
    "chat = text5\n",
    "\n",
    "\n",
    "def shorten(c,n,part):\n",
    "    mc = FreqDist(c)\n",
    "    mostCommon = [y[0] for y in mc.most_common(n)]\n",
    "    words = []\n",
    "    for m in part:\n",
    "        if m not in mostCommon:\n",
    "            words.append(m)\n",
    "    return words\n",
    "\n",
    "for x in [10,100,500]: \n",
    "    print(\"moby\",x,shorten(moby,x, moby[42:73]),\"\\n\")\n",
    "    print(\"sense and sensibility\",x, shorten(sense, x,sense[22:68]),\"\\n\")\n",
    "    print(\"chat\",x,shorten(chat,x, chat[:100]),\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Submission guidelines*\n",
    "* *The submission has to be done by a team of two people. **Individual submissions will not be graded**.*\n",
    "* *Please state the **name and matriculation number of all team members** in every submission **clearly**.*\n",
    "* *Only **one team member should submit** the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.*\n",
    "* *The submission must be in a Jupyter Notebook format (.ipynb). Submissions in **other formats will not be graded**.*\n",
    "* *It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.*\n",
    "* *If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: ``![example1](examplepicture1.PNG)`` (without apostrophs in a Markdown-Cell).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import shakespeare\n",
    "from nltk.corpus import genesis\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import brown\n",
    "from nltk import pos_tag\n",
    "from nltk.book import text1\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from sklearn import datasets, svm, tree, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">6.1.</i> :::10 Homework points:::\n",
    "</div>\n",
    "In the following, you will try out existing features for authorship attribution and implement additional ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.1.1</i> \n",
    "</div>\n",
    "\n",
    "Consider the following code, it computes 3 different features. \n",
    "Can you tell what each feature computes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeature1(text):\n",
    "    sum = 0;\n",
    "    for word in text:\n",
    "        sum+= len(word)\n",
    "    return sum/len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantCategories(text, num):\n",
    "    taggedWordList = pos_tag(text)\n",
    "    fd = nltk.FreqDist((t1, t2) for ((w1, t1), (w2, t2)) in nltk.bigrams(taggedWordList))\n",
    "    res = []\n",
    "    for (tag, freq) in fd.most_common(num):\n",
    "        res.append(tag)\n",
    "    return res\n",
    "\n",
    "def computeFeature2(text, cats):\n",
    "    taggedWordList = pos_tag(text)\n",
    "    fd = nltk.FreqDist((t1, t2) for ((w1, t1), (w2, t2)) in nltk.bigrams(taggedWordList))\n",
    "    res = []\n",
    "    for cat in cats:\n",
    "        res.append(fd.freq(cat))\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFeature3(text):\n",
    "    taggedWordList = pos_tag(text, tagset='universal')\n",
    "    fd_tags = nltk.FreqDist(tag for (word, tag) in taggedWordList)\n",
    "    fd_words = nltk.FreqDist(word for (word, tag) in taggedWordList)\n",
    "    return fd_words.freq(';')/fd_tags.freq('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">6.1.2</i> \n",
    "</div>\n",
    "\n",
    "The following code can be used to try the features on an actual classification task.<br>\n",
    "We extract features from either Shakespeare's Hamlet or Melville's Moby Dick and use them to train a classifer for distinguishing between the two authors. <br>\n",
    "We create a new dataframe. <br>\n",
    "We split each book into 300 segments with 100 tokens each. The author is the goldstandard label we want to predict.<br>\n",
    "Note that for the second feature, we first need to determine which are the most frequent categories (of what we cannot tell you, because this is what you are supposed to figure that out :) ) in the data in general to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "num = 300\n",
    "numTokens = 100\n",
    "numCategoriesToConsider = 5\n",
    "\n",
    "df['GOLD'] = (\n",
    "    [\"melville\" for i in range(0, num)]\n",
    "    +[\"shakespeare\" for i in range(0, num)]\n",
    ")\n",
    "\n",
    "df['FEATURE1'] = (\n",
    "    [computeFeature1(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)]\n",
    "    +[computeFeature1(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)]\n",
    ")\n",
    "\n",
    "allTexts = (\n",
    "    gutenberg.words('melville-moby_dick.txt')[:num*numTokens]\n",
    "    +gutenberg.words('shakespeare-hamlet.txt')[:num*numTokens]\n",
    ")\n",
    "tags = getRelevantCategories(allTexts, numCategoriesToConsider)\n",
    "\n",
    "for f in range(0, numCategoriesToConsider):\n",
    "    df['FEATURE2_'+str(f)] = (\n",
    "        [computeFeature2(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens], tags)[f] \n",
    "         for i in range(0,num)]\n",
    "        +[computeFeature2(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens], tags)[f] \n",
    "          for i in range(0,num)]\n",
    "    )\n",
    "    \n",
    "df['FEATURE3'] = (\n",
    "    [computeFeature3(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)]\n",
    "    +[computeFeature3(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)]\n",
    ")\n",
    "\n",
    "# We can inspect what the features look like\n",
    "print(df[290:310])\n",
    "\n",
    "\n",
    "# Now we train a classifier similar to what we did for Named Entity classifcation\n",
    "x = df.iloc[:, 1:len(df.columns)]\n",
    "y = df.iloc[:, [0]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "c_tree = DecisionTreeClassifier(max_depth=4)\n",
    "c_tree.fit(x_train, y_train)\n",
    "\n",
    "# ... and evaluate it\n",
    "predicted = list(c_tree.predict(x_test))\n",
    "gold = list(y_test.loc[:, \"GOLD\"])\n",
    "print(classification_report(gold,predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you extend the classifier with new features. Implement a method computeFeature4 that also takes a list of words as input and returns the average sentence length in tokens as features. <br> Integrate it into the classifier with the following code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SENTENCELENGTH'] = (\n",
    "    [computeFeature4(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)]\n",
    "    +[computeFeature4(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens]) for i in range(0,num)] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a next feature please extract the frequency of the five most frequent prepositions. First determine which are the most frequent prepositions in the dataset similar to what we saw for feature 2. The code to integrate this feature would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = getMostFrequentPreps(allTexts, 5)\n",
    "print(tags)\n",
    "for f in range(0,5):\n",
    "    df['PREP_'+str(f)] = (\n",
    "        [computeFeature5(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens], tags)[f] for i in range(0,num)]\n",
    "        +[computeFeature5(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens], tags)[f] for i in range(0,num)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the same for the most frequent function words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = getMostFrequentFunctionWords(allTexts, 5)\n",
    "print(tags)\n",
    "for f in range(0,5):\n",
    "    df['FUNCTIONWORD_'+str(f)] = (\n",
    "        [computeFeature6(gutenberg.words('melville-moby_dick.txt')[i*numTokens:(i+1)*numTokens], tags)[f] for i in range(0,num)]\n",
    "        +[computeFeature6(gutenberg.words('shakespeare-hamlet.txt')[i*numTokens:(i+1)*numTokens], tags)[f] for i in range(0,num)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the classifier again with all of the features and compare it to the original performance with the three given features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
