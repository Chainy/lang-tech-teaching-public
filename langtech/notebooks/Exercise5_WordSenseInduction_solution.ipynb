{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>/* Style des gesamten Dokuments */\n",
       "#notebook-container {\n",
       "\tfont-family: \"NimbusMonL-ReguObli\";\n",
       "\tfont-size: 120%\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r die Ãœberschrift: Zentriert diese und stellt sie fett dar. */\n",
       ".headline {\n",
       "\ttext-align: center;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 185.7%\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r die Aufgabenbeschreibung. Z.B.: \"Ãœbung zum Thema...\" */\n",
       ".description {\n",
       "\ttext-align: center;\n",
       "\tfont-size: 145.7%\n",
       "}\n",
       "\n",
       "/* Hebt das Abgabedatum fett und kursiv hervor */\n",
       "#submission {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "\n",
       "/* Style fÃ¼r das eigentliche Thema. Z.B.: \"Intelligenz\" */\n",
       "#topic {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       ".task_description {\n",
       "\tmargin-bottom: 20px;\n",
       "}\n",
       "\n",
       "/* Hebt die Aufgabennummerierung fett hervor. */\n",
       ".task {\n",
       "\tfont-style: normal;\n",
       "\tfont-weight: bold;\n",
       "\tfont-size: 120%;\n",
       "\tborder-bottom: 2px solid black;\n",
       "  background-color: #97CAEF;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 50px;\n",
       "  padding-right: 50px;\n",
       "}\n",
       "\n",
       ".subtask {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #CAFAFE;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 25px;\n",
       "  padding-right: 25px;\n",
       "}\n",
       "\n",
       ".l1 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #14A76C;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l2 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FFE400;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".l3 {\n",
       "\tfont-style: normal;\n",
       "\tfont-size: 100%;\n",
       "  background-color: #FF652F;\n",
       "  color: black;\n",
       "\tpadding: 2px;\n",
       "  padding-left: 5px;\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".points {\n",
       "\tfont-style: italic;\n",
       "}\n",
       "\n",
       "ol.lower_roman {\n",
       "    list-style-type: lower-roman;\n",
       "}\n",
       "\n",
       "ol.characters {\n",
       "    list-style-type: lower-alpha;\n",
       "}\n",
       "\n",
       "/* Style einer Code-Cell */\n",
       ".CodeMirror-code {\n",
       "\tbackground-color: #ededed\n",
       "}\n",
       "\n",
       "/* Style eines Kommentars im Code Ã¤ndern. */\n",
       ".cm-s-ipython span.cm-comment {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-atom {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-number {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Keywords Ã¤ndern */\n",
       ".cm-s-ipython span.cm-keyword {\n",
       "\tcolor: #B000B0\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-def {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Python-Variable Ã¤ndern */\n",
       ".cm-s-ipython span.cm-variable {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style einer Property Ã¤ndern */\n",
       ".cm-s-ipython span.cm-property {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Operators Ã¤ndern */\n",
       ".cm-s-ipython span.cm-operator {\n",
       "\n",
       "}\n",
       "\n",
       "/* Style eines Python-Strings Ã¤ndern */\n",
       ".cm-s-ipython span.cm-string {\n",
       "\tcolor: brown;\n",
       "}\n",
       "\n",
       "/* Style einer eingebauten Funktion Ã¤ndern (z.B. \"open\") */\n",
       ".cm-s-ipython span.cm-builtin {\n",
       "\n",
       "}\n",
       "\n",
       "/* Hebt hervor, welche Klammern zueinander passen */\n",
       ".cm-s-ipython .CodeMirror-matchingbracket {\n",
       "\n",
       "}\n",
       "\n",
       ".cm-s-ipython span.cm-variable-2 {\n",
       "\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<style>\" + open(\"style.css\").read() + \"</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"headline\">\n",
    "Language Technology / Sprachtechnologie\n",
    "<br><br>\n",
    "Wintersemester 2020/2021\n",
    "</div>\n",
    "<br>\n",
    "<div class=\"description\">\n",
    "    Übung zum Thema <i id=\"topic\">\"Word Sense Induction\"</i>\n",
    "    <br><br>\n",
    "    Deadline Abgabe: <i #id=\"submission\">Thursday, 11.12.2020 (11:55 Uhr)</i>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Präsenzübung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import wordnet as wn  \n",
    "from nltk.book import text2,text3 \n",
    "from nltk.text import Text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "from sklearn.cluster import KMeans  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.1:</i> <br>\n",
    "</div>\n",
    "\n",
    "Stopword: Which of the following statements are true?\n",
    "\n",
    "1. A stopword is a word used to stop a parsing process.\n",
    "2. A stopword is a high-frequency word.\n",
    "3. Punctuation marks like ``.``, ``,``, and ``;`` are stop words.\n",
    "4. \"the, to, and\" are stop words.\n",
    "5. Stopwords have a maximum length of 3.\n",
    "6. NLTK offers a list of English stopwords through: nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. is false. A stopword is a frequent word with little lexical content.\n",
    "2. is true. Additionally, stopwords ('a', 'the', 'to', 'also') must also carry little lexical content.\n",
    "3. is false. First, punctuation marks are not really ''words'', and are thus usually not considered stop words. In a sense, they are similar to stop words as they carry no lexical content, i.e. they have no real meaning.\n",
    "4. is true\n",
    "5. is false. There are stopwords like 'while' or 'will' with length more than 3 characters\n",
    "6. is true. There are 127 words in this list.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.2:</i> <br>\n",
    "</div>\n",
    "\n",
    "Synset: Which of the following statements are true?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A synset is a set of words that are interchangeable in some context without changing the meaning of a sentence in which they are embedded.\n",
    "2. A synset is a set of all bigrams within wordnet.\n",
    "3. Within W ordnet (corpus / lexical ressource) each word corresponds to one or more synsets.\n",
    "4. You may access the synsets of \"dog\" in NLTK by using nltk.corpus.wordnet.synsets('dog')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. is true. Synset is defined as a set of words grouped by some specific meaning. Since we need only one meaning in the context, the statement is correct.\n",
    "2. is false. Synset is a set of words and not bigrams.\n",
    "3. is true. A single word may be assigned to different synsets. For example, the word ‘car’ is assigned to such synsets as [‘car’, ‘cable_car’] and [‘car’, ‘elevator_car’].\n",
    "4. is true. The output is [Synset(‘dog.n.01’), Synset(‘frump.n.01’), Synset(‘dog.n.03’), Synset(‘cad.n.01’), Synset(‘frank.n.02’), Synset(‘pawl.n.01’), Synset(‘andiron.n.01’), Synset(‘chase.v.01’)]. ‘frump.n.01’ means that ‘dog’ belongs to the synset with the first sense of the noun (‘n’) ‘frump’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.3:</i> <br>\n",
    "</div>\n",
    "There are two possible definitions of 'word':\n",
    "\n",
    "* Word is the same as token, for example ‘see’ and ‘saw’ are different words;\n",
    "* Word is the same as lemma, hence ‘things’ and ‘thing’ becomes the same words. \n",
    "<br><br>Which of the following statements are true?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The lemma of “appeared” is“appear”.\n",
    "2. The lemma of a verb is its infinitive.\n",
    "3. Two words having the same lemma are called homonyms.\n",
    "4. Each token has one and only one lemma.\n",
    "5. Each lemma belongs to one and only one word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "1. is true, because ‘appear’ is the normalized form for ‘appeared'\n",
    "2. is true, because the infinitive is the normalized form of a verb\n",
    "3. is false. Homonyms are defined as distinct words that share the same spelling and pronunciation. However, the lemmas of these words may be different. For example, ‘goes’ and ‘went’ have both the lemma ‘go’, but are not homonyms.\n",
    "4. is false, as homographs share the same surface form, but might have different lemmas.\n",
    "5. is false, because e.g. the lemma ‘be’ corresponds to two words ‘is’ and ‘are’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.4:</i> <br>\n",
    "</div>\n",
    "\n",
    "A stopword list contains high-frequency words like “the”, “to”, “and”, or “also” that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. NLTK provides a predefined list of stopwords: nltk.corpus.stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.1</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Find all non-stopwords in carroll-alice.txt of the corpus Gutenberg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')  \n",
    "print([word for word in nltk.corpus.gutenberg.words('carroll-alice.txt') if word.lower() not in stopwords])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "It iterates over each word in carrol-alice.txt and outputs the word if its lowercase form is not inside the NLTK stopword list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.4.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Implement a function is_the_same_vocab(text1,text2) that compares two texts. It should return true, if the texts have the same vocabulary after removing the stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def get_vocab_without_stopwords(text):  \n",
    "    \"\"\"returns the vocabulary of the text without the stopwords\"\"\"  \n",
    "    return set(text) - set(nltk.corpus.stopwords.words('english'))  \n",
    "\n",
    "def is_the_same_vocab(text1, text2):  \n",
    "    \"\"\"check if the texts have the same vocabulary, after removing the stopwords -- works case sensitive\"\"\"  \n",
    "    return(get_vocab_without_stopwords(text1) == get_vocab_without_stopwords(text2))  \n",
    "\n",
    "text1 = \"People are hungry before lunch and not hungry after lunch\"  \n",
    "text2 = \"People are not hungry before lunch and hungry after lunch\"  \n",
    "text3 = \"Humans are hungry before dinner and not hungry after dinner\"  \n",
    "\n",
    "print (is_the_same_vocab(text1, text2)) # True  \n",
    "print(is_the_same_vocab(text2, text3)) # False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.5:</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below for some examples on how to work with WordNet. You can also execute it on your computer to see how the output looks like for a better understanding.\n",
    "\n",
    "Explain the code line by line, what datatype and meaning have the methods used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wn.synsets('motorcar'), \"\\n\")\n",
    "print(wn.synset('car.n.01').lemma_names(), \"\\n\")\n",
    "print(wn.synset('car.n.01').definition(), \"\\n\")\n",
    "print(wn.synset('car.n.01').examples(), \"\\n\")  \n",
    "print(wn.synset('car.n.01').lemmas(), \"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile'), \"\\n\")  \n",
    "print(wn.lemma('car.n.01.automobile').synset(), \"\\n\")\n",
    "print(wn.lemma('car.n.01.automobile').name(), \"\\n\")  \n",
    "print(wn.synsets('car'), \"\\n\")\n",
    "print(wn.lemmas('car'), \"\\n\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#returns the synsets [Synset('car.n.01')] corresponding to the word 'motorcar'.  \n",
    "print(wn.synsets('motorcar'))  \n",
    "\n",
    "# returns all words ['car', 'auto', 'automobile', 'machine', 'motorcar'] that exist in the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').lemma_names())  \n",
    "\n",
    "# returns the definition 'a motor vehicle with four wheels; usually propelled by an internal combustion engine' of the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').definition())  \n",
    "\n",
    "# returns the examples ['he needs a car to get to work'] for the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').examples())  \n",
    "\n",
    "# returns all lemmas [Lemma('car.n.01.car'), Lemma('car.n.01.auto'), Lemma('car.n.01.automobile'), Lemma('car.n.01.machine'), Lemma('car.n.01.motorcar')] that exist in the synset 'car.n.01'  \n",
    "print(wn.synset('car.n.01').lemmas())  \n",
    "\n",
    "# returns the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile'))  \n",
    "\n",
    "# returns the synset 'car.n.01' corresponding to the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile').synset())  \n",
    "\n",
    "# returns the name of the lemma 'car.n.01.automobile'  \n",
    "print(wn.lemma('car.n.01.automobile').name())  \n",
    "\n",
    "# returns the synsets [Synset('car.n.01'), Synset('car.n.02'), Synset('car.n.03'), Synset('car.n.04'), Synset('cable_car.n.01')] corresponding to the word 'car'. We note that word 'car' has more meanings than 'motorcar', hence more synsets are retrieved from WordNet.  \n",
    "print(wn.synsets('car'))  \n",
    "\n",
    "# returns the lemmas [Lemma('car.n.01.car'), Lemma('car.n.02.car'), Lemma('car.n.03.car'), Lemma('car.n.04.car'), Lemma('cable_car.n.01.car')]. 'car.n.0x.car' here are different lemmas that belong to different synsets.  \n",
    "print(wn.lemmas('car')) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.6:</i> \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "List all the senses of the word \"like\" that you can think of. Now we want to see how many different meanings the word ”like” has with the help of WordNet, using the same operations we used above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('like'):  \n",
    "    print(\"Lemma name:\\n\",synset.lemma_names())  \n",
    "    print(\"Lemma:\\n\",synset.lemmas())  \n",
    "    print(\"Definition:\\n\",synset.definition())  \n",
    "    print(\"POS:\\n:\",synset.pos())\n",
    "    print(\"Example:\\n\",synset.examples(),\"\\n____________\\n\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "As you can see above, the words can have different POS tags. Suppose you are only interested in the adjective \"warm\". Write a function that will output definitions and examples of the word \"warm\" under this condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for synset in wn.synsets('warm',wn.ADJ):  \n",
    "    print(\"Definition:\\n\",synset.definition())\n",
    "    print(\"Example:\\n\",synset.examples(),\"\\n______________\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.3.</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "Write functions that outputs the hypernyms and hyponyms of the word 'bank'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"bank\",wn.NOUN): \n",
    "    for l in syn.hypernyms(): \n",
    "            print(syn.name(),\n",
    "                  \"is a\", l.name(),\n",
    "                  \"which is\",\n",
    "                  syn.definition(),\n",
    "                  \"\\n_______\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"bank\",wn.NOUN): \n",
    "    for l in syn.hyponyms(): \n",
    "            print(syn.name(),\n",
    "                  \"has subconcept\", \n",
    "                  l.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.6.4</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Another aspect we can examine is if a word has an antonym (word with opposite meaning). Write a function that searches an antonym for the word \"like\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "for syn in wn.synsets(\"like\"): \n",
    "    for l in syn.lemmas(): \n",
    "        print(syn.name(), \"(\" , l.name() ,\")\")\n",
    "        if l.antonyms():\n",
    "            print(\"Has the antonym\", [m.name() for m in l.antonyms()], \"\\n\")\n",
    "        else: print(\"Doesn't have an antonym\\n\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.7:</i> <br>\n",
    "</div>\n",
    "The polysemy of a word is the number of senses it has. We now want to compute the average polysemy of nouns, verbs, adjectives and adverbs according to WordNet and discuss the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.1</i> <i class=\"l1\">L1</i> <br>\n",
    "</div>\n",
    "\n",
    "Try to think of an algorithm that caclulates the ploysemy of a POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>\n",
    "\n",
    "To calculate the average polysemy, we need to count every lemma of the respective POS and the number of meanings the lemma has (size of the synset). Then we divide the total number of meanings for all lemmas of the given POS by the total number of lemmas of the given POS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.2</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Take a look at the code below. Implement it and enhance the main function so that the average polysemy is calculated not only for nouns but also for verbs, adjectives and adverbs (do not implement your algorithm yet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgPolysemy():\n",
    "    #TODO, return average polysemy\n",
    "    \n",
    "avgN = avgPolysemy()\n",
    "    #TODO\n",
    "\n",
    "print(\"average polysemy of nouns: \", + avgN)\n",
    "    #TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def avgPolysemy(pos):\n",
    "    nrOfSynsets = 0\n",
    "    nrOfLemmas = 0\n",
    "    lemmas=[]\n",
    "    #TODO\n",
    "\n",
    "avgN = avgPolysemy(wn.NOUN)\n",
    "avgV = avgPolysemy(wn.VERB)\n",
    "avgADJ = avgPolysemy(wn.ADJ)\n",
    "avgADV = avgPolysemy(wn.ADV)\n",
    "\n",
    "print(\"average polysemy of nouns: \", avgN)\n",
    "print(\"average polysemy of verbs: \",  avgV)\n",
    "print(\"average polysemy of adjectives: \",  avgADJ)\n",
    "print(\"average polysemy of adverbs: \",  avgADV)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.7.3</i> <i class=\"l3\">L3</i> <br>\n",
    "</div>\n",
    "\n",
    "Now implement your algorithm and calculate the average polysemy for nouns, verbs, adjectives and adverbs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def avgPolysemy(pos):\n",
    "    nrOfSynsets = 0\n",
    "    nrOfLemmas = 0\n",
    "    lemmas=[]\n",
    "    for synset in wn.all_synsets(pos):\n",
    "        for lemma in synset.lemmas():\n",
    "            lemmas.append(lemma.name())\n",
    "    lemmas = set(lemmas)\n",
    "    for lemma in lemmas:\n",
    "        count = len(wn.synsets(lemma,pos))\n",
    "        if count>0:\n",
    "            nrOfLemmas+=1\n",
    "            nrOfSynsets+=count\n",
    "    return nrOfSynsets/nrOfLemmas\n",
    "\n",
    "avgN = avgPolysemy(wn.NOUN)\n",
    "avgV = avgPolysemy(wn.VERB)\n",
    "avgADJ = avgPolysemy(wn.ADJ)\n",
    "avgADV = avgPolysemy(wn.ADV)\n",
    "\n",
    "print(\"average polysemy of nouns: \", avgN)\n",
    "print(\"average polysemy of verbs: \",  avgV)\n",
    "print(\"average polysemy of adjectives: \",  avgADJ)\n",
    "print(\"average polysemy of adverbs: \",  avgADV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.8:</i> <br>\n",
    "</div>\n",
    "\n",
    "A concordance view shows us every occurrence of a given word, together with some context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Extract context samples of word ‘affection’ in “Sense and Sensibility” (text2) corpus of nltk and word ‘lived’ in “Book of Genesis” (text3) corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(text2.concordance(\"affection\"))\n",
    "print(text3.concordance(\"lived\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.8.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Similar words for the term can be found if words share same context as that of token. \n",
    "\n",
    "For example: monsterous occurred in contexts such as \"the monsterous pictures\" and \"a monsterous size\". What other words appear in a similar range of contexts? We can find out by appending the term similar to the name of the text in question, then inserting the relevant word in parentheses:\n",
    "\n",
    "Find out similar words for affection in “Sense and Sensibility” (text2) corpus of nltk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "text2.similar(\"affection\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.10:</i> <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are capable of capturing the meaning of words within a document. Explore the library gensim by training your own word embeddings and test pretrained word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Import the common_text corpus from the gensim library and display the first 10 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "common_texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "Train the Word2vec model based on gensim with the follwing parameters:\n",
    "    - Size of the dimension = 100\n",
    "    - Context window size = 5\n",
    "    - Minimum frequency count = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(common_texts, size=100, window=5, min_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.3.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Display the vector of the word \"computer\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model.wv['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.4.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Compute the similarity score between the words 'graph' and 'trees'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model.similarity('graph', 'trees')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.10.5.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Calculate the top 5 most similar words to the word 'graph'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "model.most_similar('graph')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with pretrained google word2vec embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "    <i class=\"task\">Task 5.11:</i> <br>\n",
    "</div>\n",
    "\n",
    "The Google word2vec embeddings were trained on Google news data (about 100 billion words); It contains 3 million words and phrases and was fit using 300-dimensional word vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GoogleNews-vectors-negative300.bin.gz embeddings in your current working directory and unzip it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 1.53 Gigabytes file. You can download it from here:\n",
    "\n",
    "https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.1.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Instantiate a gensim word2vec model using the pretrained embeddings from google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = #TODO#load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'GoogleNews-vectors-negative300.bin'\n",
    "google_word2vec_model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.11.2.</i> <i class=\"l2\">L2</i> <br>\n",
    "</div>\n",
    "\n",
    "Operation *(king – man) + woman = ?* can be performed as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_word2vec_model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the *(Germany – Berlin) + Moscow = ?* operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<strong style=\"color: blue\">Lösung</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "google_word2vec_model.most_similar(positive=['Moscow', 'Germany'], negative=['Berlin'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Submission guidelines*\n",
    "* *The submission has to be done by a team of two people. **Individual submissions will not be graded**.*\n",
    "* *Please state the **name and matriculation number of all team members** in every submission **clearly**.*\n",
    "* *Only **one team member should submit** the homework. If more than one version of the same homework is submitted by accident (submitted by more than one group member), please reach out to a tutor **as soon as possible**. Otherwise, the first submitted homework will be graded.*\n",
    "* *The submission must be in a Jupyter Notebook format (.ipynb). Submissions in **other formats will not be graded**.*\n",
    "* *It is not necessary to also submit the part of the exercise discussed by the tutor, please only submit the homework part.*\n",
    "* *If pictures need to be submitted, it is allowed to hand them in in a zip folder, together with the notebook. They should be added to the notebook like this: ``![example1](examplepicture1.PNG)`` (without apostrophs in a Markdown-Cell).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.1.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "The lexical ressource WordNet can also be used to calcualte the similarity between words.\n",
    "Use the predefined WuPalmer similarity measure in wordnet to score the similarity of each pair of words given in the code cell below. Rank the pairs in order of decreasing similarity. Print the ranked pairs as follows: *word1-word2 => score*. How close is your ranking to the order given in Miller & Charles, 1998, page 13 (Moodle)? Discuss your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that each word can have multiple synsets, which it is cointained in. For every word pair choose the corresponding pair of synsets, which maximizes the similarity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pairs = [\n",
    "    'car-automobile', 'gem-jewel', 'journey-voyage', 'boy-lad',\n",
    "    'coast-shore', 'asylum-madhouse', 'magician-wizard', 'midday-noon',\n",
    "    'furnace-stove', 'food-fruit', 'bird-cock', 'bird-crane', 'tool-implement',\n",
    "    'brother-monk', 'lad-brother', 'crane-implement', 'journey-car', 'monk-oracle',\n",
    "    'cemetery-woodland', 'food-rooster', 'coast-hill', 'forest-graveyard',\n",
    "    'shore-woodland', 'monk-slave', 'coast-forest', 'lad-wizard', 'chord-smile',\n",
    "    'glass-magician', 'rooster-voyage', 'noon-string'\n",
    "]\n",
    "\n",
    "# Example:\n",
    "# Only for demonstration the first synsets of each synset list are used.\n",
    "# This differs from task 5.1.\n",
    "s1 = wn.synsets(\"dog\",\"n\")[0]\n",
    "s2 = wn.synsets(\"cat\",\"n\")[0]\n",
    "s1.wup_similarity(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"task_description\">\n",
    "   <i class=\"subtask\">5.2.</i> :::5 Homework points:::\n",
    "</div>\n",
    "\n",
    "Now use the pretrained word embeddings (google word2vec embeddings) to calculate the similarity of each word pair. Print your solution in the same way as in task 5.1. Compare your results with your previous results (5.1) and discuss possible differences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "nlp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
